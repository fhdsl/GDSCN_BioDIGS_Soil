[["index.html", "BioDIGS: Exploring Soil Data About this Book 0.1 Target Audience 0.2 Platform 0.3 Data", " BioDIGS: Exploring Soil Data February 24, 2025 About this Book This is a companion training guide for BioDIGS, a GDSCN project that brings a research experience into the classroom. This activity guides students through exploration of the BioDIGS soil data using the tidyverse in R. Students will learn basic data summarization, visualization, and mapping skills. Visit the BioDIGS (BioDiversity and Informatics for Genomics Scholars) website here for more information about this collaborative, distributed research project, including how you can get involved! The GDSCN (Genomics Data Science Community Network) is a consortium of educators who aim to create a world where researchers, educators, and students from diverse backgrounds are able to fully participate in genomic data science research. You can find more information about its mission and initiatives here. 0.1 Target Audience The activities in this guide are written for undergraduate students and beginning graduate students. Some sections require basic understanding of the R programming language, which is indicated at the beginning of the chapter. 0.2 Platform The activities in this guide are demonstrated on NHGRI’s AnVIL cloud computing platform. AnVIL is the preferred computing platform for the GDSCN. However, all of these activities can be done using your personal installation of R or using the online Galaxy portal. 0.3 Data The data generated by the BioDIGS project is available through the BioDIGS website, as well as through an AnVIL workspace. Data about the soil itself as well as soil metal content was generated by the Delaware Soil Testing Program at the University of Delaware. Sequences were generated by the Johns Hopkins University Genetic Resources Core Facility and by PacBio. "],["background.html", "Chapter 1 Background 1.1 What is genomics? 1.2 What is data science? 1.3 What is cloud computing? 1.4 Why soil microbes? 1.5 Heavy metals and human health", " Chapter 1 Background One critical aspect of an undergraduate STEM education is hands-on research. Undergraduate research experiences enhance what students learn in the classroom as well as increase a student’s interest in pursuing STEM careers (Russell, Hancock, and McCullough 2007). It can also lead to improved scientific reasoning and increased academic performance overall (Buffalari et al. 2020). However, many students at underresourced institutions like community colleges, Historically Black Colleges and Universities (HBCUs), tribal colleges and universities, and Hispanic-serving institutions have limited access to research opportunities compared to their cohorts at larger four-year colleges and R1 institutions. These students are also more likely to belong to groups that are already under-represented in STEM disciplines, particularly genomics and data science (Canner et al. 2017; GDSCN 2022). The BioDIGS Project aims to be at the intersection of genomics, data science, cloud computing, and education. 1.1 What is genomics? Genomics broadly refers to the study of genomes, which are an organism’s complete set of DNA. This includes both genes and non-coding regions of DNA. Traditional genomics involves sequencing and analyzing the genome of individual species. Metagenomics expands genomics to look at the collective genomes of entire communities of organisms in an environmental sample, like soil. It allows researchers to study not just the genes of culturable or isolated organisms, but the entirety of genetic material present in a given environment. By using genomic techniques to survey the soil microbes, we can identify everything in the soil, including microbes that no one has identified before. We are doing both traditional genomics and metagenomics as part of BioDIGS. 1.2 What is data science? Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data. It includes collecting, cleaning, and combining data from multiple databases, exploring data and developing statistical and machine learning models to identify patterns in complex datasets, and creating tools to efficiently store, process, and access large amounts of data. 1.3 What is cloud computing? Cloud computing just means using the internet to get access to powerful computer resources like storage, servers, databases, networking tools, and specialized software programs. Instead of having to buy and maintain their own powerful computers, storage servers, and other systems, users can pay to use them through an internet connection as needed. Users only pay for what they need, when they actually use it, and professionals update and maintain the systems in large data centers. It is a particularly useful tool for researchers and students at smaller institutions with limited computational services, especially when working with complex databases. The genome assembly and analyses for BioDIGS have been done using the NHGRI AnVIL cloud computing platform, as well as Galaxy. 1.4 Why soil microbes? It can be challenging to include undergraduates in human genomic and health research, especially in a classroom context. Both human genetic data and human health data are protected data, which limits the sort of information students can access without undergoing specialized ethics training. However, the same sorts of data cleaning and analysis methods used for human genomic data are also used for microbial genomic data, which does not have the same sort of legal protections as human genetic data. This makes it ideal for training undergraduate students at the beginning of their careers and can be used to prepare students for future research in human genomics and health (Jurkowski, Reid, and Labov 2017). Additionally, the microbes in the soil can have big impacts on our health (Brevik and Burgess 2014). 1.5 Heavy metals and human health Human activities that change the landscape can also change what sorts of inorganic and abiotic compounds we find in the soil, particularly increasing the amount of heavy metals (Yan et al. 2020). When cars drive on roads, compounds from the exhaust, oil, and other fluids might settle onto the roads and be washed into the soil. When we put salt on roads, parking lots, and sidewalks, the salts themselves will eventually be washed away and enter the ecosystem through both water and soil. Chemicals from factories and other businesses also leech into our environment. Previous research has demonstrated that in areas with more human activity, like cities, soils include greater concentrations of heavy metals than found in rural areas with limited human populations (Khan et al. 2023; Wang, Birch, and Liu 2022). Increased heavy metal concentrations also disproportionately affect lower-income and predominantly minority areas (Jones et al. 2022). Research suggests that increased heavy metal concentration in soils has major impacts on the soil microbial community. In particular, increased heavy metal concentration is associated with an increase in soil bacteria that have antibiotic resistance markers (Gorovtsov, Sazykin, and Sazykina 2018; Nguyen et al. 2019; Sun, Xu, and Fan 2021). References "],["research-team.html", "Chapter 2 Research Team 2.1 Soil sampling", " Chapter 2 Research Team This project is coordinated by the Genomics Data Science Community Network (GDSCN). You can read more about the GDSCN and its mission at the network website. 2.1 Soil sampling This map shows the current sampling locations for the BioDIGS project. The extensive network of the GDSCN has made this data collection possible. Soil sampling for this project was done by both faculty and student volunteers from schools that aren’t traditional R1 research institutions. Many of the faculty are also members of the GDSCN. This list of locations reflects GDSCN institutions and friends of GDSCN who have collected soil samples. Annandale, VA: Northern Virginia Community College Atlanta, GA: Spelman College Baltimore, MD: College of Southern Maryland, Notre Dame College of Maryland, Towson University Bismark, ND: United Tribes Technical College El Paso, TX: El Paso Community College, The University of Texas at El Paso Fresno, CA: Clovis Community College Greensboro, NC: North Carolina A&amp;T State University Harrisonburg, VA: James Madison University Honolulu, Hawai’i: University of Hawai’i at Mānoa Las Cruces, NM: Doña Ana Community College Montgomery County, MD: Montgomery College, Towson University Nashville, TN: Meharry Medical College New York, NY: Guttman Community College CUNY Petersburg, VA: Virginia State University Seattle, WA: North Seattle College, Pierce College Tsaile, AZ: Diné College "],["support.html", "Chapter 3 Support 3.1 Funding 3.2 Sponsors 3.3 Analytical and Computational Support", " Chapter 3 Support This project would not be possible without financial and technical support from many organizations and people. 3.1 Funding Funding for this project has been provided by the National Human Genome Research Institute (Contract # 75N92022P00232 awarded to Johns Hopkins University). 3.2 Sponsors PacBio and CosmosID have graciously donated supplies. Advances in Genome Biology and Technology provided funding support for several team members to attend AGBT 2024. 3.3 Analytical and Computational Support Computational support has been provided by NHGRI’s AnVIL cloud computing platform and Galaxy. "],["biodigs-data.html", "Chapter 4 BioDIGS Data 4.1 Sample Metadata 4.2 Soil Property Data 4.3 Genomics and Metagenomics Data and Metadata 4.4 BioDIGSData R package", " Chapter 4 BioDIGS Data There are currently three major kinds of data available from BioDIGS: sample metadata, soil testing data, and genomics and metagenomics data. All of these are available for use in your classroom. 4.1 Sample Metadata This dataset contains information about the samples themselves, including GPS coordinates for the sample location, date the sample was taken, and the site name. This dataset is also available from the BioDIGS website You can also see images of each sampling site and soil characteristics at the sample map. 4.2 Soil Property Data This dataset includes basic information about the soil itself like pH, percentage of organic matter, variety of soil metal concentrations. The complete data dictionary is available here. The dataset is available at the BioDIGS website. This dataset was generated by the Delaware Soil Testing Program at the University of Delaware. 4.3 Genomics and Metagenomics Data and Metadata In the future, you will be able to access this data in both raw and processed forms. The Illumina and Nanopore sequences were generated at the Johns Hopkins University Genetic Resources Core Facility. PacBio sequencing was done by PacBio directly. More information coming soon! 4.4 BioDIGSData R package We’ve created a data package to help you easily bring BioDIGS soil data and metadata into R! This package is currently in development, so if there’s a feature you’d like to see, please let us know! The most up-to-date version of the package can be accessed via GitHub at https://github.com/fhdsl/BioDIGSData 4.4.1 Installation Install the package by running the following in R. You might need to install the devtools package. devtools::install_github(&quot;fhdsl/BioDIGSData&quot;) 4.4.2 Usage Bring in the data using predefined functions. For example: # Load soil property data my_data &lt;- BioDIGSData::BioDIGS_soil_data() # Load site metadata my_data &lt;- BioDIGSData::BioDIGS_metadata() # Load DNA metadata my_data &lt;- BioDIGSData::BioDIGS_DNA_conc_data() "],["billing.html", "Chapter 5 Billing 5.1 Create Google Billing Account 5.2 Add Terra to Google Billing Account 5.3 Add Members to Google Billing Account 5.4 Set Alerts for Google Billing 5.5 View Spend for Google Billing 5.6 Create Terra Billing Project 5.7 Add Member to Terra Billing Project 5.8 Disable Terra Billing Project", " Chapter 5 Billing In order to use AnVIL, you will need to set up a billing account and add members to it. These sections guide you through that process. 5.1 Create Google Billing Account Log in to the Google Cloud Platform console using your Google ID. Make sure to use the same Google account ID you use to log into Terra. If you are a first time user, don’t forget to claim your free credits! If you haven’t been to the console before, once you accept the Terms of Service you will be greeted with an invitation to “Try for Free.” Follow the instructions to sign up for a Billing Account and get your credits. Choose “Individual Account”. This “billing account” is just for managing billing, so you don’t need to be able to add your team members. You will need to give either a credit card or bank account for security. Don’t worry! You won’t be billed until you explicitly turn on automatic billing. You can view and edit your new Billing Account, by selecting “Billing” from the left-hand menu, or going directly to the billing console console.cloud.google.com/billing Clicking on the Billing Account name will allow you to manage the account, including accessing reports, setting alerts, and managing payments and billing. At any point, you can create additional Billing Accounts using the Create Account button. We generally recommend creating a new Billing Account for each funding source. 5.2 Add Terra to Google Billing Account This gives Terra permission to create projects and send charges to the Google Billing Account, and must be done by an administrator of the Google Billing Account. Terra needs to be added as a “Billing Account User”: Log in to the Google Cloud Platform console using your Google ID. Navigate to Billing You may be automatically directed to view a specific Billing Account. If you see information about a single account rather than a list of your Billing Accounts, you can get back to the list by clicking “Manage Billing Accounts” from the drop-down menu. Check the box next to the Billing Account you wish to add Terra to, click “ADD MEMBER”. Enter terra-billing@terra.bio in the text box. In the drop-down menu, mouse over Billing, then choose “Billing Account User”. Click “SAVE”. 5.3 Add Members to Google Billing Account Anyone you wish to add to the Billing Account will need their own Google ID. To add a member to a Billing Project: Log in to the Google Cloud Platform console using your Google ID. Navigate to Billing You may be automatically directed to view a specific Billing Account. If you see information about a single account rather than a list of your Billing Accounts, you can get back to the list by clicking “Manage Billing Accounts” from the drop-down menu. Check the box next to the Billing Account you wish to add a member to, click “ADD MEMBER”. Enter their Google ID in the text box. In the drop-down menu, mouse over Billing, then choose the appropriate role. Click “SAVE”. 5.4 Set Alerts for Google Billing Log in to the Google Cloud Platform console using the Google ID associated with your Google Cloud projects. Open the dropdown menu on the top left and click on Billing. You may be automatically directed to view a specific Billing Account. If you see information about a single account (and it’s not the one you’re interested in), you can get back to the list of all your Billing Accounts by clicking “Manage Billing Accounts” from the drop-down menu. Click on the name of the Billing Account you want to set alerts for. In the left-hand menu, click “Budgets &amp; alerts”. Click the “Create Budget” tab. Enter a name for your budget, and then choose which projects you want to monitor. Then click “Next”. For Budget Type, select “Specified amount”. Enter the total budget amount for the month (you will set alerts at different thresholds in the next step). Click “Next” (do not click “Finish”). Enter the threshold amounts where you want to receive an alert. We recommend starting with 50% and 90%. You can set other alerts if you prefer. Check the box for “Email alerts to billing admins and users”, then click “Finish”. Now you (as the owner and admin), along with anyone you added with admin or user privileges (e.g. lab managers) will receive alerts when your lab members reach the specified spending thresholds. These emails will be sent to the Gmail accounts associated with the Billing Account. You can edit your budgets at any time by going to Billing &gt; Budgets &amp; alerts, and clicking on the name of the budget you want to edit. 5.5 View Spend for Google Billing You can always check your current spend through the Google Billing console, but remember There is a reporting delay (~1 day), so you cannot immediately see what an analysis cost Costs are reported at the level of Workspaces, so if there are multiple people using a Workspace, you will not be able to determine which of them was responsible for the charges. The Google Billing console displays information by Billing Account. To view spending: Log in to the Google Cloud Platform console using the Google ID associated with your Google Cloud projects. Open the dropdown menu on the top left and click on Billing. You may be automatically directed to view a specific Billing Account. If you see information about a single account (and it’s not the one you’re interested in), you can get back to the list of all your Billing Accounts by clicking “Manage Billing Accounts” from the drop-down menu. Click on the name of the Billing Account for the project you want to view. Look at the top of the Overview tab to see your month-to-date spending. Scroll further down the Overview tab to show your top projects. Click on the Reports tab to see more detailed information about each of your projects. This is probably the most useful tab for exploring costs of individual projects over time. Click on the Cost table tab to obtain a convenient table of spending per project. 5.6 Create Terra Billing Project Launch Terra and sign in with your Google account. If this is your first time logging in to Terra, you will need to accept the Terms of Service. In the drop-down menu on the left, navigate to “Billing”. Click the triple bar in the top left corner to access the menu. Click the arrow next to your name to expand the menu, then click “Billing”. You can also navigate there directly with this link: https://anvil.terra.bio/#billing On the Billing page, click the “+ CREATE” button to create a new Billing Project. Select GCP Billing Project (Google’s Platform). If prompted, select the Google account to use and give Terra permission to manage Google Cloud Platform billing accounts. Enter a unique name for your Terra Billing Project and select the appropriate Google Billing Account. The name of the Terra Billing Project must: Only contain lowercase letters, numbers and hyphens Start with a lowercase letter Not end with a hyphen Be between 6 and 30 characters Select the Google Billing Account to use. All activities conducted under your new Terra Billing Project will charge to this Google Billing Account. If prompted, give Terra permission to manage Google Cloud Platform billing accounts. Click “Create”. Your new Billing Project should now show up in the list of Billing Projects Owned by You. You can add additional members or can modify or deactivate the Billing Project at any time by clicking on its name in this list. The page doesn’t always update as soon as the Billing Project is created. If it’s been a couple of minutes and you don’t see a change, try refreshing the page. 5.7 Add Member to Terra Billing Project Launch Terra and sign in with your Google account. In the drop-down menu on the left, navigate to “Billing”. Click the triple bar in the top left corner to access the menu. Click the arrow next to your name to expand the menu, then click “Billing”. You can also navigate there directly with this link: https://anvil.terra.bio/#billing Click “Owned by You” and find the Billing Project. If you do not see the Billing Project in this list, then you are not an Owner and do not have permission to add members. Click on the name of the Billing Project. Click on the “Members” tab to view and manage members. Then click the “Add User” button. Enter the email address of the user or group you’d like to add the the Billing Project. If adding an individual, make sure to enter the account that they use to access AnVIL. If adding a Terra Group, use the Group email address, which can be found on the Terra Group management page. If this user or group will need to add and remove other users of the Billing Project, check the Owner box. Otherwise leave it unchecked. It’s often a good idea to have at least one other Owner of a Billing Project in order to avoid getting locked out, in case the original owner leaves or loses access to their account. Click “ADD USER”. You should now see the user or group listed in the Billing Project members, along with the appropriate role. They should now be able to use the Billing Project to fund work on AnVIL. If you need to remove members or modify their roles, you can do so at any time by clicking the teardrop button next to their name. 5.8 Disable Terra Billing Project By default this module includes a warning to make sure people understand they will lose access to their Workspace buckets. You can remove the warning from this module by setting AnVIL_module_settings$warning to FALSE before running cow::borrow_chapter: AnVIL_module_settings &lt;- list( warning = FALSE ) cow::borrow_chapter( doc_path = &quot;child/_child_terra_billing_project_disable.Rmd&quot;, repo_name = &quot;jhudsl/AnVIL_Template&quot; ) Disabling a Billing Project makes Workspace contents inaccessible! Disabling a Billing Project disables funding to all Workspaces funded by the Billing Project. You will be unable to compute in these Workspaces, and you will lose access to any data stored in the Workspace buckets. It is sometimes possible to restore access by reactivating billing, but Google makes no promises about whether or how long the data will be recoverable. Make sure everyone with Workspaces funded by the Billing Project has saved anything they want to keep in another location before disabling the Billing Project. To disable a Terra Billing Project (i.e. remove the Google Billing Account that funds the Terra Billing Project): Launch Terra and sign in with your Google account. In the drop-down menu on the left, navigate to “Billing”. Click the triple bar in the top left corner to access the menu. Click the arrow next to your name to expand the menu, then click “Billing”. You can also navigate there directly with this link: https://anvil.terra.bio/#billing Click “Owned by You” and find the Billing Project. If you do not see the Billing Project in this list, then you are not an Owner and do not have permission to add members. Click on the name of the Billing Project. If you don’t see information about the Billing Account, click on “View billing account” to expand the Billing Account information. You may be prompted to enter your login information again. You should see the name of the Google Billing Account that is funding this Terra Billing Project. Click on the teardrop icon next to the name of the Billing Account. Click “Remove Billing Account”. Click OK to confirm that you want to disable funding for this Billing Project. The page should now indicate that there is no linked billing account. If necessary, you can restore funding to the Billing Project and associated Workspaces by clicking the teardrop icon and selecting “Change Billing Account”. However, Google makes no promises about how long the Workspace contents will remain available after you disable funding, so it is best not to rely on them. "],["anvil-workspace.html", "Chapter 6 AnVIL Workspace 6.1 Create Google Account 6.2 Create Workspace", " Chapter 6 AnVIL Workspace You can easily access the data on AnVIL by cloning the dedicated workspace. These sections guide you through creating an AnVIL account and accessing the workspace. 6.1 Create Google Account If you do not already have a Google account that you would like to use for accessing Terra, create one now. If you would like to create a Google account that is associated with your non-Gmail, institutional email address, follow these instructions. 6.2 Create Workspace Launch Terra In the drop-down menu on the left, navigate to “Workspaces”. Click the triple bar in the top left corner to access the menu. Click “Workspaces”. Click on the plus icon near the top of left of the page. Name your Workspace and select the appropriate Billing Project. All activity in the Workspace will be charged to this Billing Project (regardless of who conducted it). If you are working with protected data, you can set the Authorization Domain to limit who can be added to your Workspace. Note that the Authorization Domain cannot be changed after the Workspace is created (i.e. there is no way to make this Workspace shareable with a larger audience in the future). Workspaces by default are only visible to people you specifically share them with. Authorization domains add an extra layer of enforcement over privacy, but by nature make sharing more complicated. We recommend using Authorization Domains in cases where it is extremely important and/or legally required that the data be kept private (e.g. protected patient data, industry data). For data you would merely prefer not be shared with the world, we recommend relying on standard Workspace sharing permissions rather than Authorization Domains, as Authorization Domains can make future collaborations, publications, or other sharing complicated. Click “CREATE WORKSPACE”. The new Workspace should now show up under your Workspaces. "],["programming-platforms.html", "Chapter 7 Programming Platforms 7.1 Video overview on using RStudio 7.2 Launching RStudio 7.3 Touring RStudio 7.4 Pausing RStudio 7.5 Deleting RStudio", " Chapter 7 Programming Platforms These sections provide a general overview of how to use RStudio on AnVIL. 7.1 Video overview on using RStudio Here is a video tutorial that describes the basics of using RStudio on AnVIL. 7.1.1 Objectives Start compute for your RStudio environment Tour RStudio on AnVIL Stop compute to minimize expenses 7.1.2 Slides The slides for this tutorial are are located here. 7.2 Launching RStudio AnVIL is very versatile and can scale up to use very powerful cloud computers. It’s very important that you select a cloud computing environment appropriate to your needs to avoid runaway costs. If you are uncertain, start with the default settings; it is fairly easy to increase your compute resources later, if needed, but harder to scale down. Note that, in order to use RStudio, you must have access to a Terra Workspace with permission to compute (i.e. you must be a “Writer” or “Owner” of the Workspace). Open Terra - use a web browser to go to anvil.terra.bio In the drop-down menu on the left, navigate to “Workspaces”. Click the triple bar in the top left corner to access the menu. Click “Workspaces”. Click on the name of your Workspace. You should be routed to a link that looks like: https://anvil.terra.bio/#workspaces/&lt;billing-project&gt;/&lt;workspace-name&gt;. Click on the cloud icon on the far right to access your Cloud Environment options. If you don’t see this icon, you may need to scroll to the right. In the dialogue box, click the “Settings” button under RStudio. You will see some configuration options for the RStudio cloud environment, and a list of costs because it costs a small amount of money to use cloud computing. Configure any settings you need for your cloud environment. If you are uncertain about what you need, the default configuration is a reasonable, cost-conservative choice. It is fairly easy to increase your compute resources later, if needed, but harder to scale down. Scroll down and click the “CREATE” button when you are satisfied with your setup. The dialogue box will close and you will be returned to your Workspace. You can see the status of your cloud environment by hovering over the RStudio icon. It will take a few minutes for Terra to request computers and install software. When your environment is ready, its status will change to “Running”. Click on the RStudio logo to open a new dialogue box that will let you launch RStudio. Click the launch icon to open RStudio. This is also where you can pause, modify, or delete your environment when needed. You should now see the RStudio interface with information about the version printed to the console. 7.3 Touring RStudio Next, we will be using RStudio and the package Glimma to create interactive plots. See this vignette for more information. The Bioconductor team has created a very useful package to programmatically interact with Terra and Google Cloud. Install the AnVIL package. It will make some steps easier as we go along. You can now quickly install precompiled binaries using the AnVIL package’s install() function. We will use it to install the Glimma package and the airway package. The airway package contains a SummarizedExperiment data class. This data describes an RNA-Seq experiment on four human airway smooth muscle cell lines treated with dexamethasone. {Note: for some of the packages, you will have to install packaged from the CRAN repository, using the install.packages() function. The examples will show you which install method to use.} &lt;img src=&quot;06-using_platforms_modules_files/figure-html//1BLTCaogA04bbeSD1tR1Wt-mVceQA6FHXa8FmFzIARrg_g11f12bc99af_0_56.png&quot; alt=&quot;Screenshot of the RStudio environment interface. Code has been typed in the console and is highlighted.&quot; width=&quot;100%&quot; /&gt; Load the example data. The multidimensional scaling (MDS) plot is frequently used to explore differences in samples. When this data is MDS transformed, the first two dimensions explain the greatest variance between samples, and the amount of variance decreases monotonically with increasing dimension. The following code will launch a new window where you can interact with the MDS plot. Change the colour_by setting to “groups” so you can easily distinguish between groups. In this data, the “group” is the treatment. You can download the interactive html file by clicking on “Save As”. You can also download plots and other files created directly in RStudio. To download the following plot, click on “Export” and save in your preferred format to the default directory. This saves the file in your cloud environment. You should see the plot in the “Files” pane. Select this file and click “More” &gt; “Export” Select “Download” to save the file to your local machine. 7.4 Pausing RStudio You can view costs and make changes to your cloud environments from the panel on the far right of the page. If you don’t see this panel, you may need to scroll to the right. Running environments will have a green dot, and paused environments will have an orange dot. Hovering over the RStudio icon will show you the costs associated with your RStudio environment. Click on the RStudio icon to open the cloud environment settings. Click the Pause button to pause RStudio. This will take a few minutes. When the environment is paused, an orange dot will be displayed next to the RStudio icon. If you hover over the icon, you will see that it is paused, and has a small ongoing cost as long as it is paused. When you’re ready to resume working, you can do so by clicking the RStudio icon and clicking Resume. The right-hand side icon reminds you that you are accruing cloud computing costs. If you don’t see this icon, you may need to scroll to the right. You should minimize charges when you are not performing an analysis. You can do this by clicking on the RStudio icon and selecting “Pause”. This will release the CPU and memory resources for other people to use. Note that your work will be saved in the environment and continue to accrue a very small cost. This work will be lost if the cloud environment gets deleted. If there is anything you would like to save permanently, it’s a good idea to copy it from your compute environment to another location, such as the Workspace bucket, GitHub, or your local machine, depending on your needs. You can also pause your cloud environment(s) at https://anvil.terra.bio/#clusters. 7.5 Deleting RStudio Pausing your cloud environment only temporarily stops your work. When you are ready to delete the cloud environment, click on the RStudio icon on the right-hand side and select “Settings”. If you don’t see this icon, you may need to scroll to the right. Click on “Delete Environment”. If you are certain that you do not need the data and configuration on your disk, you should select “Delete everything, including persistent disk”. If there is anything you would like to save, open the compute environment and copy the file(s) from your compute environment to another location, such as the Workspace bucket, GitHub, or your local machine, depending on your needs. Select “DELETE”. You can also delete your cloud environment(s) and disk storage at https://anvil.terra.bio/#clusters. "],["download.html", "Chapter 8 Download", " Chapter 8 Download Coming soon! "],["introduction.html", "Chapter 9 Introduction 9.1 Before You Start 9.2 Objectives", " Chapter 9 Introduction In this activity, you’ll have a chance to become familiar with the BioDIGS soil testing data. This dataset includes information on the inorganic components of each soil sample, particularly metal concentrations. Human activity can increase the concentration of inorganic compounds in the soil. When cars drive on roads, compounds from the exhaust, oil, and other fluids might settle onto the roads and be washed into the soil. When we put salt on roads, parking lots, and sidewalks, the salts themselves will eventually be washed away and enter the ecosystem through both water and soil. Chemicals from factories and other businesses also leech into our environment. All of this means the concentration of heavy metals and other chemicals will vary among the soil samples collected for the BioDIGS project. 9.1 Before You Start This activity requires RStudio. Make sure you have access to a working version of this software. 9.2 Objectives This activity will teach you how to use the AnVIL platform to: Open data from an R package Examine objects in R Calculate summary statistics for variables in the soil testing data Create and interpret histograms and boxplots for variables in the soil testing data "],["part-1.-examining-the-data.html", "Chapter 10 Part 1. Examining the Data", " Chapter 10 Part 1. Examining the Data We will use the BioDIGSData package to retrieve the data. We first need to install the package from where it is stored on GitHub. Packages and libraries You might see or hear the term “package” or “library” when working with the R programming language. Packages are collections of R functions, data, and documentation that extend the base functionality of R. They are the fundamental units of shareable code in R. Packages are developed by the R community and made available through repositories like CRAN (Comprehensive R Archive Network), Bioconductor, and GitHub. When you install a package, you gain access to all the functions, data, and documentation provided by that package. Libraries are the directories where packages are stored. We also use the library command to load and attach packages to the R environment. When you load a package using library(package_name), you make the functions and objects from a package available for use in your current R session. You may first need to install the remotes package to your RStudio environment. remotes is a package (or chunk of pre-written code) that allows you to download code that has been stored on GitHub into RStudio. install.packages(&quot;remotes&quot;) remotes::install_github(&quot;fhdsl/BioDIGSData&quot;, upgrade = &quot;never&quot;) If you’re having trouble with the code above, you can also try: install.packages(&quot;remotes&quot;) remotes::install_url(&#39;https://github.com/fhdsl/BioDIGSData/archive/refs/tags/v1.0.0.0.tar.gz&#39;) Once you’ve installed the package, we can load the package (which just means we have access to all the data stored in the BioDIGSData package). Then we assign the soil testing data to an object. This command follows the code structure: dataset_object_name &lt;- stored_BioDIGS_dataset The “dataset_object_name” is what RStudio will call the dataset after you open it. The “stored_BioDIGS_dataset” is what the dataset is called within the BioDIGSData package. Finally, the arrow (“&lt;-”) tells R to open the “stored_BioDIGS_dataset” and save it in your environment as “dataset_object_name”. The order of these commands might be odd to our eyes, but it makes perfect sense to RStudio! The soil testing data is called BioDIGS_soil_data in the BioDIGSData package. When we save this dataset into our environment, we’re calling is soil.values. library(BioDIGSData) soil.values &lt;- BioDIGS_soil_data() It seems like the dataset loaded, but it’s always a good idea to verify. There are many ways to check, but the easiest approach (if you’re using RStudio) is to look at the Environment tab on the upper right-hand side of the screen. You should now have an object called soil.values that includes some number of observations for 28 variables. The observations refer to the number of rows in the dataset, while the variables tell you the number of columns. As long as neither the observations or variables are 0, you can be confident that your dataset loaded. Let’s take a quick look at the dataset. We can do this by clicking on soil.values object in the Environment tab. (Note: this is equivalent to typing View(soil.values) in the R console.) This will open a new window for us to scroll through the dataset. Well, the data definitely loaded, but those column names aren’t immediately understandable. What could As_EPA3051 possibly mean? In addition to the dataset, we need to load the data dictionary as well. Data dictionary: a file containing the names, definitions, and attributes about data in a database or dataset. In this case, the data dictionary can help us make sense of what sort of values each column represents. The data dictionary for the BioDIGS soil testing data is available in the R package (see code below), but we have also reproduced it here. ?BioDIGS_soil_data() collection_date: Date sample was collected (soil was removed from a site). site_id: Unique letter and number site name. Check BioDIGS_metadata() for GPS coordinates, origin, and more. sample_id: Unique sequencing sample identifier. site_name_rep_detail: Detailed label for the sample, intended to help disambiguate in case of confusion. As_EPA3051: Arsenic (mg/kg), EPA Method 3051A. Quantities &lt; 3.0 are not detectable. Cd_EPA3051: Cadmium (mg/kg), EPA Method 3051A. Quantities &lt; 0.2 are not detectable. Cr_EPA3051: Chromium (mg/kg), EPA Method 3051A Cu_EPA3051: Copper (mg/kg), EPA Method 3051A Ni_EPA3051: Nickel (mg/kg), EPA Method 3051A Pb_EPA3051: Lead (mg/kg), EPA Method 3051A Zn_EPA3051: Zinc (mg/kg), EPA Method 3051A water_pH: Water pH OM_by_LOI_pct: Organic Matter by Loss on Ignition P_Mehlich3: Phosphorus (mg/kg), using the Mehlich 3 soil test extractant K_Mehlich3: Potassium (mg/kg), using the Mehlich 3 soil test extractant Ca_Mehlich3: Calcium (mg/kg), using the Mehlich 3 soil test extractant Mg_Mehlich3: Magnesium (mg/kg), using the Mehlich 3 soil test extractant Mn_Mehlich3: Manganese (mg/kg), using the Mehlich 3 soil test extractant Zn_Mehlich3: Zinc (mg/kg), using the Mehlich 3 soil test extractant Cu_Mehlich3: Copper (mg/kg), using the Mehlich 3 soil test extractant Fe_Mehlich3: Iron (mg/kg), using the Mehlich 3 soil test extractant B_Mehlich3: Boron (mg/kg), using the Mehlich 3 soil test extractant S_Mehlich3: Sulfur (mg/kg), using the Mehlich 3 soil test extractant Na_Mehlich3: Sodium (mg/kg), using the Mehlich 3 soil test extractant Al_Mehlich3: Aluminum (mg/kg), using the Mehlich 3 soil test extractant Est_CEC: Cation Exchange Capacity (meq/100g) at pH 7.0 (CEC) Base_Sat_pct: Base saturation (BS). This represents the percentage of CEC occupied by bases (Ca2+, Mg2+, K+, and Na+). The %BS increases with increasing soil pH (Figure 5). The availability of Ca2+, Mg2+, and K+ increases with increasing %BS. P_Sat_ratio: Phosphorus saturation ratio. This is the ratio between the amount of phosphorus present in the soil and the total capacity of that soil to retain phosphorus. The ability of phosphorus to be bound in the soil is primary a function of iron (Fe) and aluminum (Al) content in that soil. Using the data dictionary, we find that the values in column As_EPA3051 give us the arsenic concentration in mg/kg of each soil sample, as determined by EPA Method 3051A. This method uses a combination of heat and acid to extract specific elements (like arsenic, cadmium, chromium, copper, nickel, lead, and zinc) from soil samples. While arsenic can occur naturally in soils, higher levels suggest the soil may have been contaminated by mining, hazardous waste, or pesticide application. Arsenic is toxic to humans. QUESTIONS: What data is found in the column labeled “Fe_Mehlich3”? Why would we be interested how much of this is in the soil? (You may have to search the internet for this answer.) What data is found in the column labeled “Base_Sat_pct”? What does this variable tell us about the soil? You may notice that some cells in the soil.values table contain NA. This just means that the soil testing data for that sample isn’t available yet. We’ll take care of those values in the next part. QUESTIONS: How many observations are in the soil testing values dataset that you loaded? What do each of these observations refer to? How many different soil characteristics are in the dataset? How can you tell? "],["part-2.-summarizing-with-statistics.html", "Chapter 11 Part 2. Summarizing with Statistics", " Chapter 11 Part 2. Summarizing with Statistics Now that we have the dataset loaded, let’s explore the data in more depth. First, we should remove those samples that don’t have soil testing data yet. We could keep them in the dataset, but removing them at this stage will make the analysis a little cleaner. In this case, as we know the reason the data are missing (and that reason will not skew our analysis), we can safely remove these samples. This will not be the case for every data analysis. We can remove the unanalyzed samples using the drop_na() function from the tidyverse package. The tidyverse package is a collection of code that makes organizing data tables (also known as data wrangling) easy in R. The drop_na function removes any rows from a table that contains NA for a particular column. This command follows the code structure: dataset_new_name &lt;- dataset_object_name %&gt;% drop_na(column_name) The %&gt;% is called a pipe and it tells R that the commands after it should all be applied to the object in front of it. (In this case, we can filter out all samples missing a value for “As_EPA3051” as a proxy for samples without soil testing data.) The following code block opens the tidyverse package, then tells R to filter out all samples missing a value for “As_EPA3051” (as a proxy for samples without soil testing data) from the soil.values object, and finally to save the new, filtered dataset as an object called soil.values.clean. #install.packages(&#39;tidyverse&#39;) if you haven&#39;t already installed the tidyverse! library(tidyverse) soil.values.clean &lt;- soil.values %&gt;% drop_na(As_EPA3051) Great! Now let’s calculate some basic statistics. For example, we might want to know what the mean (average) arsenic concentration is for all the soil samples. We can use a combination of two functions: pull() and mean(). pull() lets you extract a column from your table for statistical analysis, while mean() calculates the average value for the extracted column. This command follows the code structure: object_name %&gt;% pull(column_name) %&gt;% mean() For this chunk of code, R will calculate the mean for the AS_EPA3051 values in the soil.values.clean dataset. Because we have not used the &lt;- to save the mean in a new object, R will simply display the mean. soil.values.clean %&gt;% pull(As_EPA3051) %&gt;% mean() ## [1] 5.10875 We can run similar commands to calculate the standard deviation (sd), minimum (min), and maximum (max) for the soil arsenic values. soil.values.clean %&gt;% pull(As_EPA3051) %&gt;% sd() ## [1] 5.606926 soil.values.clean %&gt;% pull(As_EPA3051) %&gt;% min() ## [1] 0 soil.values.clean %&gt;% pull(As_EPA3051) %&gt;% max() ## [1] 27.3 The soil testing dataset contains samples from multiple geographic regions, so maybe it’s more meaningful to find out what the average arsenic values are for each region. However, first we need to grab a new dataset from the BioDIGSData package that tells us the geographic regions of each sample. Let’s open BioDIGS_metadata, save it as soil.meta, and look at it. soil.meta &lt;- BioDIGS_metadata() View(soil.meta) The metadata (or, data about the samples) contains information stored as 7 different variables. We can see that this dataset contains a variable called site_id that matches a column in the soil.values and soil.values.clean datasets. This is important! Using this variable, we can combine the soil.values.clean and soil.meta into a single dataset. The command for combining the datasets follows the code structure: combined_dataset_name &lt;- first_dataset %&gt;% inner_join(second_dataset, by = column_name_in_common) The inner_join command tells R to combine the first_dataset and the second_dataset based on matching information in the column_name_in_column. It also only keeps those rows that are found in both datasets. Finally, the &lt;- tells R to save this combined dataset as a new object. soil.combined &lt;- soil.values.clean %&gt;% inner_join(soil.meta, by = &quot;site_id&quot;) View(soil.combined) When you scroll through the soil.combined dataset, you now see the metadata columns after all the soil characteristics. In particular, there’s a column called origin which gives the town or city location for each sample. Many of the samples from the pilot study came from 5 places in Maryland: Baltimore, Derwood, Boyds, Germantown, and Bethesda. Let’s look at the average arsenic content of the soil for each of these locations! We have to do a little bit of clever coding trickery for this using the group_by and summarize functions. First, we tell R to split our dataset up by a particular column (in this case, origin) using the group_by function, then we tell R to summarize the mean arsenic concentration for each group. When using the summarize function, we tell R to make a new table (technically, a tibble in R) that contains two columns: the column used to group the data and the statistical measure we calculated for each group. This command follows the code structure: dataset %&gt;% group_by(column_name) %&gt;% summarize(mean(column_name)) soil.combined %&gt;% group_by(origin) %&gt;% summarize(mean(As_EPA3051)) ## # A tibble: 5 × 2 ## origin `mean(As_EPA3051)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 Baltimore, MD 5.56 ## 2 Bethesda, MD 3.46 ## 3 Boyds, MD 6.94 ## 4 Derwood, MD 4.26 ## 5 Germantown, MD 4.30 Now we know that the mean arsenic concentration might be different for each region of origin. QUESTIONS: All the samples in the initial pilot study were collected from public park land. Some parks were located in suburban and rural areas, while others were collected from urban parks. Why might soil arsenic concentration be different for rural parks than for urban parks? What is the mean iron concentration for samples in this dataset? What about the standard deviation, minimum value, and maximum value? Calculate the mean iron concentration by region of origin. Which region has the highest mean iron concentration? What about the lowest? Let’s say we’re interested in looking at mean concentration for any element that was determined using EPA Method 3051. Given that there are 8 of these measures in the soil.combined dataset, it would be time consuming to run our code from above that calculates overall mean for each individual measure. We can add two arguments to our summarize statement to calculate statistical measures for multiple columns at once: the across argument, which tells R to apply the summarize command to multiple columns; and the ends_with parameter, which tells R which columns should be included in the statistical calculation. We are using ends_with because for this question, all the columns that we’re interested in end with the string ‘EPA3051’. This command follows the code structure: dataset %&gt;% group_by(column_name) %&gt;% summarize(across(ends_with(common_column_name_ending), mean)) soil.combined %&gt;% group_by(origin) %&gt;% summarize(across(ends_with(&#39;EPA3051&#39;), mean)) ## # A tibble: 5 × 8 ## origin As_EPA3051 Cd_EPA3051 Cr_EPA3051 Cu_EPA3051 Ni_EPA3051 Pb_EPA3051 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Baltimore, … 5.56 0.359 34.5 35.0 17.4 67.2 ## 2 Bethesda, MD 3.46 0.375 67.5 17.0 26.6 41.8 ## 3 Boyds, MD 6.94 0.0525 16.8 16.8 12.9 31.6 ## 4 Derwood, MD 4.26 0.335 39.5 31.3 35.1 42.1 ## 5 Germantown,… 4.30 0.602 19.9 23.3 17.7 38.2 ## # ℹ 1 more variable: Zn_EPA3051 &lt;dbl&gt; This is a much more efficient way to calculate statistics. QUESTIONS: Calculate the maximum values for concentrations that were determined using EPA Method 3051. (HINT: change the function you call in the summarize statement.) Which of these metals has the maximum concentration you see, and in which region is it found? Calculate both the mean and maximum values for concentrations that were determined using the Mehlich3 test. (HINT: change the terms in the the part of the code that uses ends_with, as well as the function you call in the summarize statement.) Which of these metals has the highest average and maximum concentrations, and in which region are they found? "],["part-3.-visualizing-the-data.html", "Chapter 12 Part 3. Visualizing the Data", " Chapter 12 Part 3. Visualizing the Data Often, it can be easier to immediately interpret data displayed as a plot than as a list of values. For example, we can more easily understand how the arsenic concentration of the soil samples are distributed if we create histograms compared to looking at point values like mean, standard deviation, minimum, and maximum. One way to make histograms in R is with the hist() function. This function only requires that we tell R which column of the dataset that we want to plot. (However, we also have the option to tell R a histogram name and a x-axis label.) We can again use the pull() command and pipes (%&gt;%) to choose the column we want from the soil.values.clean dataset and make a histogram of them. This combination of commands follows the code structure: dataset %&gt;% pull(column_name) %&gt;% hist(main = chart_title, xlab = x_axis_title) soil.combined %&gt;% pull(As_EPA3051) %&gt;% hist(main = &#39;Histogram of Arsenic Concentration&#39;, xlab =&#39;Concentration in mg/kg&#39; ) We can see that almost all the soil samples had very low concentrations of arsenic (which is good news for the soil health!). In fact, many of them had arsenic concentrations close to 0, and only a few sampling locations appear to have high levels of arsenic. We might also want to graphically compare arsenic concentrations among the geographic regions in our dataset. We can do this by creating boxplots. Boxplots are particularly useful when comparing the mean, variation, and distributions among multiple groups. In R, one way to create a boxplot is using the boxplot() function. We don’t need to use pipes for this command, but instead will specify what columns we want to use from the dataset inside the boxplot() function itself. This command follows the code structure: boxplot(column_we’re_plotting ~ grouping_variable, data = dataset, main = “Title of Graph”, xlab = “x_axis_title”, ylab = “y_axis_title”) boxplot(As_EPA3051 ~ origin, data = soil.combined, main = &quot;Arsenic Concentration by Geographic Region&quot;, xlab = &quot;Region&quot;, ylab = &quot;Arsenic Concentration in mg/kg&quot;) By using a boxplot, we can quickly see that, while two sampling sites within Baltimore, MD have a very high concentration of arsenic in the soil (indicated by the two small circles on the plot), in general there isn’t a difference in arsenic content between any of our locations. QUESTIONS: Create a histogram for iron concentration, as well as a boxplot comparing iron concentration by region. Is the iron concentration similar among regions? Are there any outlier sites with unusually high or low iron concentrations? Create a histogram for lead concentration, as well as a boxplot comparing lead concentration by region. Is the lead concentration similar among regions? Are there any outlier sites with unusually high or low lead concentrations? "],["activity-questions.html", "Chapter 13 Activity Questions 13.1 Part 1. Examining the Data 13.2 Part 2. Summarizing the Data with Statistics 13.3 Part 3. Visualizing the Data", " Chapter 13 Activity Questions 13.1 Part 1. Examining the Data What data is found in the column labeled “Fe_Mehlich3”? Why would we be interested how much of this is in the soil? (You may have to search the internet for this answer.) What data is found in the column labeled “Base_Sat_pct”? What does this variable tell us about the soil? How many observations are in the soil testing values dataset that you loaded? What do each of these observations refer to? How many different soil characteristics are in the dataset? How can you tell? 13.2 Part 2. Summarizing the Data with Statistics All the samples in the initial pilot study were collected from public park land. Some parks were located in suburban and rural areas, while others were collected from urban parks. Why might soil arsenic concentration be different for rural parks than for urban parks? What is the mean iron concentration for samples in this dataset? What about the standard deviation, minimum value, and maximum value? Calculate the mean iron concentration by region. Which region has the highest mean iron concentration? What about the lowest? Calculate the maximum values for concentrations that were determined using EPA Method 3051. (HINT: change the function you call in the summarize statement.) Which of these metals has the maximum concentration you see, and in which region is it found? Calculate both the mean and maximum values for concentrations that were determined using the Mehlich3 test. (HINT: change the terms in the columns_to_include vector, as well as the function you call in the summarize statement.) Which of these metals has the highest average and maximum concentrations, and in which region are they found? 13.3 Part 3. Visualizing the Data Create a histogram for iron concentration, as well as a boxplot comparing iron concentration by region. Is the iron concentration similar among regions? Are there any outlier sites with unusually high or low iron concentrations? Create a histogram for lead concentration, as well as a boxplot comparing lead concentration by region. Is the lead concentration similar among regions? Are there any outlier sites with unusually high or low lead concentrations? "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Content Developer Elizabeth Humphries Content Editors Ava Hoffman, Kate Isaac Project Directors Ava Hoffman, Michael Schatz, Jeff Leek, Frederick Tan Production Content Publisher Ira Gooding Technical Template Publishing Engineers Candace Savonen, Carrie Wright, Ava Hoffman Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottrpal) John Muschelli, Candace Savonen, Carrie Wright Package Developer (BioDIGSData) Ava Hoffman Funding Funder National Human Genome Research Institute (NHGRI) Funding Staff Fallon Bachman, Jennifer Vessio, Emily Voeglein   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.3.2 (2023-10-31) ## os Ubuntu 22.04.4 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2025-02-24 ## pandoc 3.1.1 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## bookdown 0.41 2024-10-16 [1] CRAN (R 4.3.2) ## bslib 0.6.1 2023-11-28 [1] RSPM (R 4.3.0) ## cachem 1.0.8 2023-05-01 [1] RSPM (R 4.3.0) ## cli 3.6.2 2023-12-11 [1] RSPM (R 4.3.0) ## devtools 2.4.5 2022-10-11 [1] RSPM (R 4.3.0) ## digest 0.6.34 2024-01-11 [1] RSPM (R 4.3.0) ## ellipsis 0.3.2 2021-04-29 [1] RSPM (R 4.3.0) ## evaluate 0.23 2023-11-01 [1] RSPM (R 4.3.0) ## fastmap 1.1.1 2023-02-24 [1] RSPM (R 4.3.0) ## fs 1.6.3 2023-07-20 [1] RSPM (R 4.3.0) ## glue 1.7.0 2024-01-09 [1] RSPM (R 4.3.0) ## htmltools 0.5.7 2023-11-03 [1] RSPM (R 4.3.0) ## htmlwidgets 1.6.4 2023-12-06 [1] RSPM (R 4.3.0) ## httpuv 1.6.14 2024-01-26 [1] RSPM (R 4.3.0) ## jquerylib 0.1.4 2021-04-26 [1] RSPM (R 4.3.0) ## jsonlite 1.8.8 2023-12-04 [1] RSPM (R 4.3.0) ## knitr 1.48 2024-07-07 [1] CRAN (R 4.3.2) ## later 1.3.2 2023-12-06 [1] RSPM (R 4.3.0) ## lifecycle 1.0.4 2023-11-07 [1] RSPM (R 4.3.0) ## magrittr 2.0.3 2022-03-30 [1] RSPM (R 4.3.0) ## memoise 2.0.1 2021-11-26 [1] RSPM (R 4.3.0) ## mime 0.12 2021-09-28 [1] RSPM (R 4.3.0) ## miniUI 0.1.1.1 2018-05-18 [1] RSPM (R 4.3.0) ## pkgbuild 1.4.3 2023-12-10 [1] RSPM (R 4.3.0) ## pkgload 1.3.4 2024-01-16 [1] RSPM (R 4.3.0) ## profvis 0.3.8 2023-05-02 [1] RSPM (R 4.3.0) ## promises 1.2.1 2023-08-10 [1] RSPM (R 4.3.0) ## purrr 1.0.2 2023-08-10 [1] RSPM (R 4.3.0) ## R6 2.5.1 2021-08-19 [1] RSPM (R 4.3.0) ## Rcpp 1.0.12 2024-01-09 [1] RSPM (R 4.3.0) ## remotes 2.4.2.1 2023-07-18 [1] RSPM (R 4.3.0) ## rlang 1.1.4 2024-06-04 [1] CRAN (R 4.3.2) ## rmarkdown 2.25 2023-09-18 [1] RSPM (R 4.3.0) ## sass 0.4.8 2023-12-06 [1] RSPM (R 4.3.0) ## sessioninfo 1.2.2 2021-12-06 [1] RSPM (R 4.3.0) ## shiny 1.8.0 2023-11-17 [1] RSPM (R 4.3.0) ## stringi 1.8.3 2023-12-11 [1] RSPM (R 4.3.0) ## stringr 1.5.1 2023-11-14 [1] RSPM (R 4.3.0) ## urlchecker 1.0.1 2021-11-30 [1] RSPM (R 4.3.0) ## usethis 2.2.3 2024-02-19 [1] RSPM (R 4.3.0) ## vctrs 0.6.5 2023-12-01 [1] RSPM (R 4.3.0) ## xfun 0.48 2024-10-03 [1] CRAN (R 4.3.2) ## xtable 1.8-4 2019-04-21 [1] RSPM (R 4.3.0) ## yaml 2.3.8 2023-12-11 [1] RSPM (R 4.3.0) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
